{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_NUM_TREES=300\n",
    "RF_MAX_DEPTH=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import findspark\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds and adds spark to python path\n",
    "# Convenient for env managers like conda\n",
    "\n",
    "#findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC, LogisticRegression\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, FloatType, ArrayType, StringType, DoubleType\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an initial spark configuration utilizing all local cores\n",
    "conf = SparkConf().setMaster(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates spark context through which to process RDD ops\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName(\"Word Count\")\\\n",
    "                    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline for csv-->formatted dataframe\n",
    "def mold(df, labeled=True,numPartitions=48):\n",
    "    # Select relevent id+features\n",
    "    df = df.select([df.columns[1]]+df.columns[145:])\n",
    "\n",
    "    # Rename and recast id\n",
    "    df = df.withColumn(df.schema.names[0],col(df.schema.names[0]).cast(\"Long\")).withColumnRenamed(\"Face ID\", \"face_id\")\n",
    "\n",
    "    offset= 2 if labeled else 1\n",
    "\n",
    "    # Rename and recast features\n",
    "    for i in range(len(df.schema.names)-offset):\n",
    "        df = df.withColumn(df.schema.names[1+i],col(df.schema.names[1+i]).cast(\"Float\"))\n",
    "  \n",
    "    # Rename and recast labels (if appropriate)\n",
    "    if labeled:\n",
    "        df = df.withColumn('Y',col(df.schema.names[-1]).cast(\"Integer\")).drop('Sex (subj)')\n",
    "    \n",
    "    return spark.createDataFrame(df.rdd.repartition(numPartitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates handlers for spark-loaded files. Since spark uses lazy execution,\n",
    "# this process occurs at no true cost and will only read/load memory when\n",
    "# a downstream task requires it\n",
    "bucket='gs://uga-dsp/project2/files/'\n",
    "#_train=spark.read.load(f\"{bucket}X_small_train.csv\", format=\"csv\", header=True)\n",
    "_test=spark.read.load(f\"{bucket}X_small_test.csv\", format=\"csv\", header=True)\n",
    "big_train=spark.read.load(f\"{bucket}X_train.csv\", format=\"csv\", header=True)\n",
    "_testA=spark.read.load(f\"{bucket}Xa_test.csv\", format=\"csv\", header=True)\n",
    "_testB=spark.read.load(f\"{bucket}Xb_test.csv\", format=\"csv\", header=True)\n",
    "_testC=spark.read.load(f\"{bucket}Xc_test.csv\", format=\"csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads either training set (or both). Uncomment as needed\n",
    "\n",
    "#trainingData=mold(_train)\n",
    "#del trainingData\n",
    "#trainingDataBig=mold(big_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads test sets. Uncomment as needed\n",
    "testingData=mold(_test)\n",
    "testingDataFinal={'a':mold(_testA,False),\n",
    "                  'b':mold(_testB,False),\n",
    "                  'c':mold(_testC,False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(train,model_type='rf'):\n",
    "    \n",
    "    # Include a bias column for logistic regression\n",
    "    if model_type=='lr':\n",
    "        train=train.withColumn(\"bias\", lit(1)).select([train.schema.names[0],'bias']+train.schema.names[1:])\n",
    "\n",
    "\n",
    "    # Assembler for compiling features into a singular dense vector    \n",
    "    train_assembler = VectorAssembler().setInputCols(train.schema.names[1:-1]).setOutputCol('features')\n",
    "\n",
    "    # Assembled training data    \n",
    "    trainData=train_assembler.transform(train).selectExpr('face_id','features',\"Y\")\n",
    "\n",
    "    # Index labels, adding metadata to the label column.\n",
    "    # Fit on whole dataset to include all labels in index.\n",
    "    labelIndexer = StringIndexer(inputCol=\"Y\", outputCol=\"indexedLabel\").fit(trainData)\n",
    "\n",
    "    # Create model templates.\n",
    "    rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=RF_NUM_TREES,maxDepth=RF_MAX_DEPTH)\n",
    "    \n",
    "    gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\",maxDepth=RF_MAX_DEPTH, maxIter=100)\n",
    "\n",
    "    layers = [len(train.schema.names[1:-1]), 256, 256, 2]\n",
    "    perceptron = MultilayerPerceptronClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\",maxIter=400, layers=layers, blockSize=128)\n",
    "\n",
    "    lsvc = LinearSVC(labelCol=\"indexedLabel\", featuresCol=\"features\",maxIter=40, regParam=0.1)\n",
    "\n",
    "    lr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"features\",maxIter=400, regParam=0.0, elasticNetParam=0)\n",
    "\n",
    "    # Convert indexed labels back to original labels.\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                                   labels=labelIndexer.labels)\n",
    "\n",
    "    # Dictionary of model types for easy selection and extension\n",
    "    _model={'rf':rf,'gbt':gbt,'per':perceptron,'svm':lsvc,'lr':lr}[model_type]\n",
    "\n",
    "    # Chain indexers and chosen model in a Pipeline\n",
    "    pipeline = Pipeline(stages=[labelIndexer, _model, labelConverter])\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainData)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(test,model,model_type='rf',labeled=True):\n",
    "\n",
    "    # Include a bias column for logistic regression\n",
    "    if model_type=='lr':\n",
    "        test=test.withColumn(\"bias\", lit(1)).select([test.schema.names[0],'bias']+test.schema.names[1:])\n",
    "\n",
    "    # List of features to compile\n",
    "    _names=test.schema.names[1:-1] if labeled else test.schema.names[1:]\n",
    "    test_assembler = VectorAssembler().setInputCols(_names).setOutputCol('features')\n",
    "\n",
    "    # Format of test data, depending on whether it is labeled\n",
    "    cols=['face_id','features']\n",
    "    if labeled:\n",
    "        cols+=['Y']\n",
    "    testData=test_assembler.transform(test).select(*cols)\n",
    "\n",
    "    # Apply model to data to form prediction\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    out_cols=['face_id','predictedLabel']\n",
    "    if labeled:\n",
    "        out_cols+=['Y']\n",
    "    return predictions.select(*out_cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model=PipelineModel.load('gs://micky-practicum/rf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%time model=buildModel(trainingDataBig,model_type='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=testModel(testingDataFinal['a'],model,model_type='rf',labeled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = output.withColumn(\"predictedLabel\", output[\"predictedLabel\"].cast(IntegerType())).withColumn(\"face_id\", output[\"face_id\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(False,0.05,seed=0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rdd.coalesce(1).saveAsTextFile(\"gs://micky-practicum/ya.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.map(lambda x:int(x[1])).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_count=output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_output=output.map(lambda x:int(x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def saveResults(model):\n",
    "    for s in ['a','b','c']:\n",
    "        output=testModel(testingDataFinal[s],model,model_type='rf',labeled=False)\n",
    "        dest=f'y{s}.txt'\n",
    "        with open(dest, 'a') as the_file:\n",
    "            for row in output.collect():\n",
    "                the_file.write(f'{row[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval(out):\n",
    "    count=0\n",
    "    for o in out:\n",
    "        if int(o[1])==o[2]:\n",
    "            count+=1\n",
    "    return count/len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(output.filter(lambda x:int(x[1])==x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('gs://micky-practicum/rf_output_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()/output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output. Make sure to name appropriately\n",
    "\n",
    "dest='yc.txt'\n",
    "with open(dest, 'a') as the_file:\n",
    "    for row in output.collect():\n",
    "        the_file.write(f'{row[1]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
